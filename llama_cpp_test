!pip install langchain
!CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir

from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

llm = LlamaCpp(
	# model_path: 로컬머신에 다운로드 받은 모델의 위치
    model_path="/workspace/llama-2-13b-chat.Q4_K_M.gguf",
    temperature=0.0,
    top_p=1,
    max_tokens=8192,
    verbose=True,
    # n_ctx: 모델이 한 번에 처리할 수 있는 최대 컨텍스트 길이
    n_ctx=4096,
    # GPU 사용하려면 n_gpu_layers가 꼭 필요하다........................
    n_gpu_layers=43,
    callback_manager=callback_manager
)


# def call_LlamaCPP() :
#     n_gpu_layers = 1
#     n_batch = 512
#     n_ctx = 2048
#     temperature = 0
#     top_p=1
#     callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) # Callbacks support token-wise streaming

#     return LlamaCpp(
#         model_path="/workspace/llama-2-13b-chat.Q4_K_M.gguf",
#         n_gpu_layers=n_gpu_layers,
#         n_batch=n_batch,
#         f16_kv=True,
#         callback_manager=callback_manager,
#         verbose=True,
        
#         n_ctx=n_ctx,
#         temperature = temperature,
#         top_p = top_p
#     )

# llm = call_LlamaCPP()
